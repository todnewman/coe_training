{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Classifier - keras+tensorboard.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/todnewman/coe_training/blob/master/MNIST_Classifier_keras+tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sCXfQsYeeqhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generic Convolutional Neural Network example with Tensorboard Visualizations\n",
        "\n",
        "This example is intended to provide a very simple solution to MNIST classification using Keras (a convenient front-end for Tensorflow).  Additionally, it demonstrates how to connect to Tensorboard through CoLaboratory.\n",
        "\n",
        "MNIST is the hand-written digits dataset created by NIST and later modified to fit in a 28x28 pixel bounding box and anti-aliased to include grayscale.  It has been used for years to evaluate Image processing and machine learning algorithms.  The CNN is the current champion, with error rates as low as 0.2%.\n",
        "\n",
        "Author: W. Tod Newman\n",
        "Learning Objectives:\n",
        "* Learn how to set up a simple Tensorflow model \n",
        "* Learn how to simplify the Tensorflow model definition using Keras\n",
        "* Understand how to connect to Tensorboard and how to evaluate metrics in Tensorboard\n",
        "* Understand how to use this example with a custom image dataset\n",
        "\n",
        "\n",
        "## Tensorboard Overview\n",
        "The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, Google has included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.\n",
        "\n",
        "Here's a good overview of what you can do with Tensorboard.  I'd watch the video on this link first.  https://www.tensorflow.org/guide/summaries_and_tensorboard\n"
      ]
    },
    {
      "metadata": {
        "id": "vE3bA_BGUBX7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# First, we need to connect to Tensorboard from CoLaboratory\n",
        "As you probably know, your Google Colab virtual machine is running on a local network located in a Google's server room, while your local machine could be anywhere else in the world.\n",
        "\n",
        "How then can we access the TensorBoard page from our local machine?\n",
        "\n",
        "Fortunately there's a free service named ngrok that will allow us to connect to the Tensorboard connection on our Web browser.\n",
        "\n",
        "## Step One: Get the ngrok image from its stable location, then unzip it. \n",
        "Remember, this is all being done in the Google environment, so don't worry about your firewall...\n",
        "ngrok is pretty useful for testing.  Find more at https://ngrok.com/"
      ]
    },
    {
      "metadata": {
        "id": "DZO3EUaPea1E",
        "colab_type": "code",
        "outputId": "eef8ab15-673e-47f9-eca5-a2f40a18f840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package ngrok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xxlyLTryeAzM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step Two: Run TensorBoard"
      ]
    },
    {
      "metadata": {
        "id": "b0wdo5o8dyzm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Fmv_Kj4ewmv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step Three: Run ngrok"
      ]
    },
    {
      "metadata": {
        "id": "SRi8c-KQePas",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_8v4ZK5ezdU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step Four:  Grab the URL that the Tensorboard instance will be hosted at.\n",
        "The URL below (in blue) is what you click on to see your Tensorboard instance after you train the CNN."
      ]
    },
    {
      "metadata": {
        "id": "LYvGuEaMeu-C",
        "colab_type": "code",
        "outputId": "53859cad-dc7e-4a4f-d5bd-a5658a7190f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://2bee512a.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aus-alWQe_lD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MNIST Classifer in Keras with Tensorboard metrics\n",
        "\n",
        "This is a standard MNIST classifier.  It classifies over 99% at 12 epochs (an epoch is a complete pass through the training data), and that's even without tweaking hyperparameters.  After running this, click on the Tensorboard link in the block above this (something like http://***.ngrok.io)"
      ]
    },
    {
      "metadata": {
        "id": "4pxUfiLhbS4Y",
        "colab_type": "code",
        "outputId": "da087777-fe47-4173-e216-364972f6154d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions (in this case, MNIST images)\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# First, we load the mnist data.  This is a built-in call, as MNIST is such\n",
        "# a common dataset to use to test image classifiers.  Then we will take\n",
        "# the data, shuffle it and split it between train and test sets\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Evaluate which backend data format we have.  Tensorflow has its own backend,\n",
        "# but some people use Theano or other backends that have different ways of \n",
        "# structuring the image data. This has to do with efficiency on CPUs/GPUs.\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "    \n",
        "# Here we cast the image data to a 32 bit float and normalize.  This improves\n",
        "# neural net performance.\n",
        "# INFO: The color components of an 8-bit RGB image are integers in the \n",
        "# range [0, 255] rather than floating-point values in the range [0, 1]. \n",
        "# A pixel whose color components are (255,255,255) is displayed as white. \n",
        "# To take this to a normalized floating point value, therefore, we divide \n",
        "# by 255.\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Will print out shape of the data in NHWC format, where N refers to the number \n",
        "# of images in a batch, H refers to the number of pixels in the vertical \n",
        "# dimension, W refers to the number of pixels in the horizontal dimension, \n",
        "# and C refers to the channels (e.g. 1 for black and white, 3 for RGB, etc.)\n",
        "\n",
        "print('x_train shape in NHWC format:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define our model below.  We have two 2D convolution layers [32,64]\n",
        "# followed by pooling (we do this to control overfitting) and a 128 neuron\n",
        "# fully-connected layer.  We're using dropout here to randomly reduce the \n",
        "# network complexity.  The classification layer is a Softmax.  \n",
        "# The Adadelta optimizer adapts learning rates based on a moving window of \n",
        "# gradient updates, which has the effect of extending learning longer.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Call TensorBoard for visual metrics\n",
        "\n",
        "tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=batch_size,\n",
        "                         write_images=True)\n",
        "\n",
        "# Fit the model based on everything we've defined to date.  Write metrics to \n",
        "# TensorBoard\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[tbCallBack])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape in NHWC format: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 12s 203us/step - loss: 0.2679 - acc: 0.9176 - val_loss: 0.0600 - val_acc: 0.9811\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0863 - acc: 0.9746 - val_loss: 0.0392 - val_acc: 0.9868\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0655 - acc: 0.9804 - val_loss: 0.0322 - val_acc: 0.9885\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0520 - acc: 0.9844 - val_loss: 0.0356 - val_acc: 0.9879\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0450 - acc: 0.9865 - val_loss: 0.0286 - val_acc: 0.9913\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0404 - acc: 0.9873 - val_loss: 0.0294 - val_acc: 0.9902\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0383 - acc: 0.9882 - val_loss: 0.0254 - val_acc: 0.9911\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0350 - acc: 0.9893 - val_loss: 0.0259 - val_acc: 0.9914\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0324 - acc: 0.9902 - val_loss: 0.0258 - val_acc: 0.9912\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0299 - acc: 0.9910 - val_loss: 0.0236 - val_acc: 0.9923\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0299 - acc: 0.9908 - val_loss: 0.0242 - val_acc: 0.9919\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0273 - acc: 0.9919 - val_loss: 0.0269 - val_acc: 0.9925\n",
            "Test loss: 0.02689936623844178\n",
            "Test accuracy: 0.9925\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}