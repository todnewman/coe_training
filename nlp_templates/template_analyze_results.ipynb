{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/todnewman/coe_training/blob/master/nlp_templates/template_analyze_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analysis Notebook\n",
        "Does various analysis of the text\n",
        "## Functions:\n",
        "1. process_text(): Takes the Knowledge Graph Dataframe and creates an instance of the know_graph class. Then offers various options within the know_graph class (LDA, Named Entity, Document Summarization).\n",
        "2. print_kg():\n",
        "3. inference_kg()\n",
        "4. most_central_nodes()"
      ],
      "metadata": {
        "id": "iXtzNt2T5Ou7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if IS_MASTER exists, this variable will only exist if it's being called by MASTER notebook.\n",
        "# if it does not exist, set it to False\n",
        "try: IS_MASTER\n",
        "except: IS_MASTER = False"
      ],
      "metadata": {
        "id": "PCGFClmUxJDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH-PbuElPqJz",
        "outputId": "e01a8409-c2ea-4b0a-e7cc-13ad824a6140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import bs4\n",
        "import requests\n",
        "import glob\n",
        "import networkx as nx\n",
        "from networkx.convert_matrix import from_numpy_array\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd0V_YCwPyh3",
        "outputId": "d6b32be7-0e4d-40ad-c3eb-36ac15e75067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if not IS_MASTER:\n",
        "    #\n",
        "    # Set params for standalone mode\n",
        "    #\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT_PATH = f\"/content/drive/My Drive/SCR-Analytics/angie/pima/housing_reports/output/\"\n",
        "    OUTPUT_DIR = ROOT_PATH\n",
        "    SYMBOL = \"ukraine\"\n",
        "    os.chdir(ROOT_PATH)\n",
        "\n",
        "    import sys\n",
        "    sys.path.insert(0,'/content/drive/My Drive/Libraries')\n",
        "    import gentext\n",
        "    import gengraph\n",
        "    import know_graph\n",
        "\n",
        "    SUMMARIZE_DOC = True\n",
        "    PROCESS_LDA = False\n",
        "    NE_FLAG = True\n",
        "    NUM_TOPICS_TO_SHOW = 5\n",
        "    HIER_CLUSTERS = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzt0N1okPzKP"
      },
      "source": [
        "## Begin Processing Text from saved text file outputs\n",
        "\n",
        "* Create instance of gentext class\n",
        "* Process LDA algorithm to reveal Topics.  Right now I have 8 topics defined, but a closer look may need to be taken on individual docs to see if that prior fits.\n",
        "* Summarize the document by finding most central sentences (30 right now).  Save these most central sentences to a text file for later processing.\n",
        "* Find named entities.  In the future one could go into the graph and figure out how each named entity contributes to the knowledge graph.\n",
        "* Heirarchical Clustering: Along with LDA can help the analyst identify key themes and topics.\n",
        "* Build knowledge graph from most central sentences.  Since we're limiting to 30 sentences, this might provide a cliff notes of the document?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4yNJakoPz9R"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_text(fnm, kg_df):\n",
        "    '''\n",
        "    Function: process_text()\n",
        "\n",
        "    Description:\n",
        "\n",
        "    Returns:\n",
        "        text_processing: The instance of the GenText class\n",
        "        topics: the gensim model method print_topics()\n",
        "        opt_mod_topics\n",
        "        dominant_topic\n",
        "        corpus\n",
        "        top_sent\n",
        "    '''\n",
        "    #with open(fnm, 'r', encoding='utf-8', errors='ignore') as raw_data:\n",
        "    print(fnm)\n",
        "    label = f'Evaluation of {fnm}'\n",
        "    ignore_words = ['http', 'www', 'org', 'com', 'pdf', 'http:']\n",
        "    out_file = f\"{fnm}_txtout.txt\"\n",
        "    top_sent_file = f\"{fnm[0:5]}_top_sents_centrality.txt\"\n",
        "\n",
        "    text_processing = gentext.GenText(\n",
        "                                    fname = fnm,\n",
        "                                    ignore_words=ignore_words,\n",
        "                                    chunk_size = 100000,\n",
        "                                    outfile=out_file,\n",
        "                                    bigrams=False,\n",
        "                                    label=label,\n",
        "                                    verbose=False)\n",
        "\n",
        "    if SUMMARIZE_DOC:\n",
        "        #\n",
        "        #  This is a nice summary method using sentence centrality.  We pass the\n",
        "        #  GenText member function (summarize_with_vectors) the number of sentences we\n",
        "        #  want to rank.  This is also a way to generate a list of \"interesting\" sentences\n",
        "        #  to use later as input to a different algorithm.\n",
        "        #\n",
        "        nx_graph, sentences_clean, ranked_sent, top_sent = text_processing.summarize_with_vectors(500)\n",
        "\n",
        "        for i,s in enumerate(top_sent[0:20]):\n",
        "            print(f\"{i}: {s}\")\n",
        "\n",
        "        with open(top_sent_file, 'w') as f:\n",
        "            for item in top_sent:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "\n",
        "    if NE_FLAG:\n",
        "        #\n",
        "        #  Named entities can be interesting.  Future work could build graphs of\n",
        "        #  NE's and their neighbors to do something like stakeholder evaluation?\n",
        "        #\n",
        "        named_entities = text_processing.process_NE()\n",
        "        print(f\"\\nNamed Entities: {named_entities}\")\n",
        "\n",
        "    if HIER_CLUSTERS:\n",
        "        #\n",
        "        # These are mildly interesting.  Sometimes evaluation of the heirarchical\n",
        "        # clusters along with LDA topics yields unique insight.\n",
        "        #\n",
        "        hier_cluster_words = text_processing.hier_clustering(8,15)\n",
        "        for w in hier_cluster_words:\n",
        "            print (w)\n",
        "\n",
        "    if PROCESS_LDA:\n",
        "        topics, sents, opt_mod_topics, dominant_topic, corpus = text_processing.process_LDA_gensim(num_topics=8, num_words=20)\n",
        "\n",
        "    return (text_processing, topics, opt_mod_topics, dominant_topic, corpus, top_sent)\n",
        "\n",
        "def most_central_nodes(G, num, kg_top_sents, fnm, verbose=False):\n",
        "    cent_arr = []\n",
        "\n",
        "    #\n",
        "    # Build an array of \"significant\" sentral nodes.  If shorter than 3, usually oddities like\n",
        "    # numbers and if longer than 25 a web address.\n",
        "    #\n",
        "    for a, data in sorted(G.nodes(data=True), key=lambda x: x[1]['betweenness'], reverse=True):\n",
        "        if verbose:\n",
        "            print('{a} {w}'.format(a=a,  w=data['betweenness']))\n",
        "        if len(a) > 3 and len(a) < 25:\n",
        "            cent_arr.append(a)\n",
        "        if len(cent_arr) == num: # Limit to desired array length\n",
        "            break\n",
        "\n",
        "    text_file = f'{OUTPUT_DIR}{SYMBOL}{fnm}.txt'\n",
        "    #\n",
        "    # One thought on the below is to filter the top sentences by some keyword\n",
        "    # so we don't always get complex sentences that don't address questions\n",
        "    #\n",
        "    with open(text_file, \"r\") as txt_vals:\n",
        "        sents = nltk.sent_tokenize(''.join(txt_vals))\n",
        "        sent_nos = []\n",
        "        for n in cent_arr:\n",
        "            fil_n = kg_top_sents['Subject'] == n\n",
        "            centr_sents = kg_top_sents[fil_n]['sentno'].unique()\n",
        "\n",
        "            for i, s in enumerate(centr_sents):\n",
        "                sent_nos.append(s)\n",
        "        sent_nos = set(sent_nos)\n",
        "        sent_out = []\n",
        "        for i,s in enumerate(sent_nos):\n",
        "            sent_out.append(sents[s])\n",
        "\n",
        "    return cent_arr, sent_out\n",
        "\n",
        "\n",
        "def plot_top_topics(top_topics, num_topics_to_print, fnm):\n",
        "    for i,t in enumerate(top_topics):\n",
        "        plt.figure(figsize=(15,10))\n",
        "        top_topics_data = t[0]\n",
        "        top_topics_coherence = t[1]\n",
        "        plt.bar(range(len(top_topics_data)), [val[0] for val in top_topics_data], align='center')\n",
        "        plt.title(f\"Topic {i}, Coherence: {t[1]}\", fontsize=18)\n",
        "        plt.xticks(range(len(top_topics_data)), [val[1] for val in top_topics_data])\n",
        "        plt.xticks(rotation=70, fontsize = 16)\n",
        "        plt.ylabel('Probability of Topic Inclusion', fontsize = 16)\n",
        "        plt.xlabel(f'Words Describing Topic {i}', fontsize = 16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{TOPIC_DIR}LDA_{fnm}_Topic_{i}.png',dpi=300, bbox_inches = \"tight\")\n",
        "        plt.show()\n",
        "        if i == num_topics_to_print:\n",
        "            break\n",
        "\n",
        "def most_common_words(text, num):\n",
        "\n",
        "    # tokenize\n",
        "    raw = ' '.join(word_tokenize(text.lower()))\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r'[A-Za-z]{2,}')\n",
        "    words = tokenizer.tokenize(raw)\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # count word frequency, sort and return the specified number\n",
        "    counter = Counter()\n",
        "    counter.update(words)\n",
        "    most_common = counter.most_common(num)\n",
        "    return most_common\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhbDbdAYRYaI"
      },
      "source": [
        "## Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "M-535OmuPznp",
        "outputId": "4c6df3f3-a940-4848-be51-eef6e21bd8d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8eeea11aa246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#  Main Function below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# We want to deserialize the /content/tmp/*pkl files here instead of opening files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "#\n",
        "#  Main Function below.\n",
        "#\n",
        "#os.chdir(OUTPUT_DIR)\n",
        "#\n",
        "# We want to deserialize the /content/tmp/*pkl files here instead of opening files\n",
        "# Maybe this presents problems for going file by file though...  perhaps need\n",
        "# to serialize by filename too?\n",
        "# The sentences*txt files didn't get serialized though, so maybe this is OK\n",
        "#\n",
        "\n",
        "all_files_rollup = SEP_DOCS # Only perform LDA on the rolled up set of files, not individually\n",
        "\n",
        "if not IS_MASTER:\n",
        "    filenames = glob.glob(f\"{SYMBOL}_all_files*.txt\")\n",
        "else:\n",
        "    if not all_files_rollup:\n",
        "        filenames = glob.glob(f\"{OUTPUT_DIR}{SYMBOL}_all_files.txt\")\n",
        "    else:\n",
        "        filenames = glob.glob(f\"{OUTPUT_DIR}{SYMBOL}_*.txt\")\n",
        "\n",
        "files = [i for i in filenames]\n",
        "docs_arr = []\n",
        "debug = False\n",
        "nodeval_arr = []\n",
        "top_sent_LDA = False # True if we want to run LDA on the most central sentence data.\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# Simple function to Flatten an array of arrays\n",
        "#\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "for fnm in files:\n",
        "    #\n",
        "    # UPDATE the BELOW for different headers\n",
        "    #\n",
        "    print(f'Opening file: {fnm}')\n",
        "    fnm_split = fnm.split('.')[0]\n",
        "    actual_fnm = fnm_split.rsplit('/',1)[-1] # Gents rid of header info and extension\n",
        "    print(f\"ACTUAL_FNM: {actual_fnm}\")\n",
        "    print(f\"FILENAME WITH PATH: {fnm}\")\n",
        "    #\n",
        "    # Open up the Knowledge Graph Dataframe\n",
        "    #\n",
        "    if not IS_MASTER:\n",
        "        kg_fnm = f'{SYMBOL}_kg_df{actual_fnm}.csv'\n",
        "        kg_df = pd.read_csv(kg_fnm)\n",
        "    else:\n",
        "        kg_df = pd.read_pickle(PROCESSED_KGDF_FILE)\n",
        "\n",
        "    print(\"Knowledge Graph DF info:\", kg_df.columns, len(kg_df))\n",
        "    #\n",
        "    #  Call function that opens an instance of the know_graph class.\n",
        "    #\n",
        "    text_processing, topics, opt_model, dominant_topic, corpus, top_sents = process_text(fnm,\n",
        "                                                                                         kg_df)\n",
        "    #\n",
        "    # Save off the top sentences by centrality to the output folder.  This can function\n",
        "    # as a summary of the document.\n",
        "    #\n",
        "    with open(f'{OUTPUT_DIR}{SYMBOL}_top_sentences.txt', \"w\") as txt_file:\n",
        "            for line in top_sents:\n",
        "                txt_file.write(\"\".join(line) + \"\\n\")\n",
        "\n",
        "    #\n",
        "    # Based off our Gensim LDA algorithm, plot the number of topics desired\n",
        "    #\n",
        "    top_topics = opt_model.top_topics(corpus=corpus)\n",
        "    plot_top_topics(top_topics, NUM_TOPICS_TO_SHOW, actual_fnm)\n",
        "\n",
        "    #\n",
        "    # Save off and show the dominant topic per sentence\n",
        "    #\n",
        "    dominant_topic.to_csv(f\"{TOPIC_DIR}{SYMBOL}_dom_topic_{actual_fnm}.csv\")\n",
        "    print(\"\\nDominant Topic Table\")\n",
        "    display(dominant_topic)\n",
        "    if top_sent_LDA:\n",
        "        #\n",
        "        #  Call function that opens an instance of the know_graph class for the most central sentences.\n",
        "        #\n",
        "        top_sent_fnm = f'{OUTPUT_DIR}{SYMBOL}_top_sentences.txt'\n",
        "        text_processing, topics, opt_model, dominant_topic, corpus, top_sents = process_text(top_sent_fnm, kg_df)\n",
        "        NUM_TOPICS_TO_SHOW = 4\n",
        "        actual_fnm = \"Most_Central_Sentences\"\n",
        "\n",
        "        #\n",
        "        # Based off our Gensim LDA algorithm, plot the number of topics desired\n",
        "        #\n",
        "        top_topics = opt_model.top_topics(corpus=corpus)\n",
        "        plot_top_topics(top_topics, NUM_TOPICS_TO_SHOW, actual_fnm)\n",
        "\n",
        "        #\n",
        "        # Save off and show the dominant topic per sentence\n",
        "        #\n",
        "        dominant_topic.to_csv(f\"{OUTPUT_DIR}{SYMBOL}_dom_topic_{actual_fnm}.csv\")\n",
        "        dominant_topic\n",
        "    print(\"Most Common Words from the Top Sentences\")\n",
        "    common_vals = most_common_words(' '.join(top_sents), 20)\n",
        "    df_cv = pd.DataFrame(common_vals, columns=['word', 'frequency'])\n",
        "    display(df_cv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR7zfA_5IpBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a9116f-b6ad-4bcf-bc12-f188b14eff13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1s4-Bt5nCUDG1Gqt7O9pQtSK4IuDREsVO/Reports for Tod\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKkgdtaA0geQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBwiRqw7bADWBW4h1+FBcV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}